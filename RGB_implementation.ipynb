{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nl9EMktBLFXW"
      },
      "outputs": [],
      "source": [
        "#Install and upgrade the necessary libraries for Hugging Face models, LangChain, OpenAI, and Gradio.\n",
        "\n",
        "!pip -q install --upgrade huggingface_hub\n",
        "!pip -q install langchain_community\n",
        "!pip -q install langchain_huggingface\n",
        "!pip install huggingface_hub\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install groq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7kZHpXUl0rq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "#Allows interaction with the operating system.\n",
        "import os\n",
        "#Provides access to system-specific parameters and functions.\n",
        "import sys\n",
        "#Imports the OpenAI Python library for interacting with OpenAI models like GPT-4.\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "#Provides support for reading configuration files (.ini).\n",
        "import configparser\n",
        "#Imports Hugging Face Endpoint support from LangChain.\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "#Imports GenerationConfig, which is used to configure text generation parameters.\n",
        "from transformers import GenerationConfig\n",
        "#A popular library for making HTTP requests.\n",
        "import requests\n",
        "#Imports TQDM, a library for progress bars.\n",
        "import tqdm\n",
        "#Enables Google Drive integration in Google Colab\n",
        "from google.colab import drive\n",
        "#Provides pre-trained tokenizer and model utilities for text generation.\n",
        "#AutoTokenizer: Loads the appropriate tokenizer for a given model.\n",
        "#AutoModel: Loads a general-purpose model.\n",
        "#AutoModelForCausalLM: Loads a causal language model (LLM) for text generation.\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import random, math\n",
        "import gradio as gr\n",
        "import groq\n",
        "from groq import Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GkQkYlpdg73"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files\n",
        "# 'force_remount=True' ensures that the drive is unmounted and remounted if already mounted\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "\n",
        "# Define the base folder path where configuration files are stored in Google Drive\n",
        "# Ensure that the folder \"RGB_Data_Config\" exists in your Google Drive\n",
        "RGB_Config_folder_path = \"/content/drive/My Drive/RGB_Data_Config/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koANUqm94SnL"
      },
      "outputs": [],
      "source": [
        "def read_config(config_file_path):\n",
        "    # Check if the configuration file exists at the given path\n",
        "    if not os.path.exists(config_file_path):\n",
        "        # Raise an error if the file doesn't exist\n",
        "        raise FileNotFoundError(f\"Configuration file not found: {config_file_path}\")\n",
        "\n",
        "    # Initialize a ConfigParser object to read and handle the INI configuration file\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    try:\n",
        "        # Attempt to read the configuration file\n",
        "        config.read(config_file_path)\n",
        "    except Exception as e:\n",
        "        # If reading the file fails, raise an error with details\n",
        "        raise ValueError(f\"Failed to read configuration file: {config_file_path}. Error: {e}\")\n",
        "\n",
        "    # Return the parsed configuration object\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drdz05K21t2g"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Define the path to the debug file. This file will be used for logging debug information.\n",
        "debug_file_name = RGB_Config_folder_path + \"debug.txt\"\n",
        "\n",
        "# Read the configuration from a specified INI file located in the folder defined by RGB_Config_folder_path.\n",
        "# The configuration is stored in the `Config` object for later use.\n",
        "Config = read_config(RGB_Config_folder_path + 'config.ini')\n",
        "\n",
        "# Extract the Hugging Face API key from the 'Settings' section of the configuration file.\n",
        "hfapi_key = Config.get('Settings', 'hfapi_key')\n",
        "\n",
        "# Extract the Groq API key from the 'Settings' section of the configuration file.\n",
        "#groq_key = \"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\" # Config.get('Settings', 'groq_key')\n",
        "groq_key = \"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\"\n",
        "\n",
        "# Extract the OpenAI API key from the 'Settings' section of the configuration file.\n",
        "openai_api_key = Config.get('Settings', 'openai_api_key')\n",
        "\n",
        "# Define the URL for OpenAI's chat completions API.\n",
        "open_api_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "# Define the number of questions to be used or processed.\n",
        "# This could represent a batch size or a limit in certain operations.\n",
        "Number_of_questions = 300\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDKolFHaDnQA"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Embedding LLM configuration\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    # Initialization of the class with model and optional URL\n",
        "    def __init__(self, model, url=None):\n",
        "        self.model = model  # Set the model name\n",
        "        self.set_hf_login()  # Set up Hugging Face authentication\n",
        "        # Initialize the InferenceClient with the model\n",
        "        self.client = InferenceClient(self.model)\n",
        "\n",
        "        # Create HuggingFace LLM endpoint with specific configurations for text generation\n",
        "        self.LLM1 = HuggingFaceEndpoint(\n",
        "            repo_id=model,  # Model repo\n",
        "            task=\"text-generation\",  # Task type is text generation\n",
        "            max_new_tokens=1024,  # Limit on new tokens generated\n",
        "            temperature=0.1,  # Controls randomness of generation\n",
        "            top_k=30,  # Limits sampling to top 30 tokens\n",
        "            repetition_penalty=1.03  # Penalty for repeating text\n",
        "        )\n",
        "\n",
        "    # Method to set Hugging Face login by managing environment variables\n",
        "    def set_hf_login(self):\n",
        "        if \"HF_TOKEN\" in os.environ:\n",
        "            del os.environ[\"HF_TOKEN\"]  # Remove existing token if present\n",
        "        if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ:\n",
        "            del os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]  # Same for the second token\n",
        "        os.environ[\"HF_TOKEN\"] = hfapi_key  # Set new token\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hfapi_key  # Set token for Hugging Face Hub\n",
        "\n",
        "    # Method to generate a response from the Hugging Face model\n",
        "    def generate(self, text, system=\"\", temperature=0.7, top_p=1.0):\n",
        "        headers = {\"Authorization\": f\"Bearer {hfapi_key}\"}  # Bearer token for authentication\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": f\"{system}\\n{text}\",  # Combine system instruction and user input\n",
        "            \"parameters\": {\n",
        "                \"temperature\": temperature,  # Sampling temperature\n",
        "                \"top_p\": top_p,  # Nucleus sampling\n",
        "                \"return_full_text\": False  # Only return the generated text\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Use Hugging Face model endpoint to generate the text based on the provided inputs\n",
        "        response = self.LLM1.invoke(f\"{system}\\n{text}\")\n",
        "        print(response)  # Print the generated response for debugging purposes\n",
        "\n",
        "        return response  # Return the generated response\n",
        "\n",
        "\n",
        "class GroqModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.client = groq.Groq(api_key=groq_key)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate(self, prompt, temperature, system=None):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"system\", \"content\": system or \"\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# OpenAI LLM configuration\n",
        "class OpenAILLM:\n",
        "    # Initialization with model and max token limit for OpenAI LLM\n",
        "    def __init__(self, QA_Model, max_tokens):\n",
        "        openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "        self.API_KEY = openai_api_key  # Store the key for API calls\n",
        "        self.Open_AI_LLM_inst = OpenAI(api_key=openai_api_key)  # Create an OpenAI instance\n",
        "        self.model = QA_Model  # Model name\n",
        "        self.max_tokens = max_tokens  # Maximum token limit for OpenAI API response\n",
        "        self.url = open_api_url  # OpenAI API URL\n",
        "\n",
        "    # Method to generate a response from the OpenAI model\n",
        "    def generate(self, text: str, temperature=0.7, system=\"You are a helpful assistant. You can help me by answering my questions. You can also ask me questions.\", top_p=1):\n",
        "        headers = {\"Authorization\": f\"Bearer {self.API_KEY}\"}  # Bearer token for OpenAI API\n",
        "\n",
        "        query = {\n",
        "            \"model\": self.model,  # Model to use for text generation\n",
        "            \"temperature\": temperature,  # Sampling temperature for randomness\n",
        "            \"top_p\": top_p,  # Nucleus sampling parameter\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system},  # System message defining the role\n",
        "                {\"role\": \"user\", \"content\": text}  # User input text\n",
        "            ],\n",
        "            \"stream\": False  # Stream option to get the response in chunks (set to False for a single response)\n",
        "        }\n",
        "\n",
        "        responses = requests.post(self.url, headers=headers, json=query)  # Send the request to OpenAI API\n",
        "\n",
        "        if 'choices' not in responses.json():\n",
        "            print(text)  # Print the input if no valid response is returned\n",
        "            print(responses)  # Print the full API response for debugging\n",
        "\n",
        "        # Print and return the generated response from OpenAI\n",
        "        print(responses.json()['choices'][0]['message']['content'])\n",
        "        return responses.json()['choices'][0]['message']['content']\n",
        "\n",
        "    # Function to get a specific OpenAI response to a prompt\n",
        "    def get_openai_response(self, prompt):\n",
        "        try:\n",
        "            # Prepare the message list with prompt and test question\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": prompt},  # System instruction\n",
        "                {\"role\": \"user\", \"content\": \"test question?\"},  # User query\n",
        "            ]\n",
        "\n",
        "            # Use OpenAI LLM's beta chat completion API\n",
        "            response = self.Open_AI_LLM_inst.beta.chat.completions.parse(\n",
        "                model=self.QA_Model, messages=messages, max_tokens=self.max_tokens\n",
        "            )\n",
        "\n",
        "            # Return the content of the first choice (answer) from the OpenAI response\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors and return an error message\n",
        "            return f\"Error: {e}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef0_3KHxg3yR"
      },
      "outputs": [],
      "source": [
        "# Function to process the data for a given instance based on noise rate, passage number, filename, and correctness rate.\n",
        "def processdata(instance, noise_rate, passage_num, filename, correct_rate):\n",
        "    # Extract the query and answer from the instance.\n",
        "    query = instance['query']\n",
        "    ans = instance['answer']\n",
        "\n",
        "    # Calculate the number of negative and positive samples based on the noise rate.\n",
        "    neg_num = math.ceil(passage_num * noise_rate)\n",
        "    pos_num = passage_num - neg_num\n",
        "\n",
        "    # Case when the filename contains '_int' (could indicate some internal data or instruction-based format)\n",
        "    if '_int' in filename:\n",
        "        # Shuffle the positive samples\n",
        "        for i in instance['positive']:\n",
        "            random.shuffle(i)\n",
        "        print(len(instance['positive']))  # Debugging output: print the length of positive samples\n",
        "        # Select the first document from each positive set\n",
        "        docs = [i[0] for i in instance['positive']]\n",
        "\n",
        "        # If there are not enough positive documents, fill in with additional documents from larger positive sets\n",
        "        if len(docs) < pos_num:\n",
        "            maxnum = max([len(i) for i in instance['positive']])\n",
        "            for i in range(1, maxnum):\n",
        "                for j in instance['positive']:\n",
        "                    if len(j) > i:\n",
        "                        docs.append(j[i])\n",
        "                        if len(docs) == pos_num:\n",
        "                            break\n",
        "                if len(docs) == pos_num:\n",
        "                    break\n",
        "        # Calculate the number of negative documents needed\n",
        "        neg_num = passage_num - len(docs)\n",
        "        # Add negative documents if needed\n",
        "        if neg_num > 0:\n",
        "            negative = instance['negative'][:neg_num]\n",
        "            docs += negative\n",
        "    # Case when the filename contains '_fact' (likely for factual questions or true/false type tasks)\n",
        "    elif '_fact' in filename:\n",
        "        # Calculate the number of correct documents to select based on the correct rate.\n",
        "        correct_num = math.ceil(passage_num * correct_rate)\n",
        "        pos_num = passage_num - neg_num - correct_num\n",
        "        # Randomly select positive examples\n",
        "        indexs = list(range(len(instance['positive'])))\n",
        "        selected = random.sample(indexs, min(len(indexs), pos_num))\n",
        "        docs = [instance['positive_wrong'][i] for i in selected]\n",
        "\n",
        "        # Remaining indices after selecting positive examples\n",
        "        remain = [i for i in indexs if i not in selected]\n",
        "        # Add correct examples if available\n",
        "        if correct_num > 0 and len(remain) > 0:\n",
        "            docs += [instance['positive'][i] for i in random.sample(remain, min(len(remain), correct_num))]\n",
        "\n",
        "        # Add negative examples if needed\n",
        "        if neg_num > 0:\n",
        "            docs += instance['negative'][:neg_num]\n",
        "    else:\n",
        "        # General case for other types of data (no '_int' or '_fact' in filename)\n",
        "        if noise_rate == 1:\n",
        "            neg_num = passage_num  # All passages are negative if noise_rate is 1\n",
        "            pos_num = 0\n",
        "        else:\n",
        "            # Adjust the number of positive and negative passages based on the available data\n",
        "            if neg_num > len(instance['negative']):\n",
        "                neg_num = len(instance['negative'])\n",
        "                pos_num = passage_num - neg_num\n",
        "            elif pos_num > len(instance['positive']):\n",
        "                pos_num = len(instance['positive'])\n",
        "                neg_num = passage_num - pos_num\n",
        "\n",
        "        # Select the required number of positive and negative samples\n",
        "        positive = instance['positive'][:pos_num]\n",
        "        negative = instance['negative'][:neg_num]\n",
        "\n",
        "        # Combine the selected positive and negative samples\n",
        "        docs = positive + negative\n",
        "\n",
        "    # Shuffle the final document list\n",
        "    random.shuffle(docs)\n",
        "\n",
        "    return query, ans, docs  # Return the query, answer, and the processed documents\n",
        "\n",
        "\n",
        "# Function to check if the predicted answer matches the ground truth.\n",
        "def checkanswer(prediction, ground_truth):\n",
        "    prediction = prediction.lower()  # Convert prediction to lowercase for case-insensitive comparison\n",
        "    if type(ground_truth) is not list:\n",
        "        ground_truth = [ground_truth]  # Ensure ground_truth is always a list\n",
        "\n",
        "    labels = []\n",
        "    for instance in ground_truth:\n",
        "        flag = True\n",
        "        if type(instance) == list:  # If the ground truth is a list of possible answers\n",
        "            flag = False\n",
        "            instance = [i.lower() for i in instance]  # Lowercase all possible answers\n",
        "            for i in instance:\n",
        "                if i in prediction:\n",
        "                    flag = True  # Set flag to True if any possible answer is found in prediction\n",
        "                    break\n",
        "        else:\n",
        "            instance = instance.lower()  # Lowercase the single ground truth answer\n",
        "            if instance not in prediction:\n",
        "                flag = False  # Set flag to False if the ground truth is not found in prediction\n",
        "        labels.append(int(flag))  # Convert boolean flag to integer (1 for correct, 0 for incorrect)\n",
        "\n",
        "    return labels  # Return the list of labels (1 for correct, 0 for incorrect)\n",
        "\n",
        "\n",
        "# Function to evaluate the results, checking for correctness based on a threshold.\n",
        "def getevalue(results):\n",
        "    results = np.array(results)  # Convert results to a numpy array for easier manipulation\n",
        "    results = np.max(results, axis=0)  # Take the max along axis 0 (likely across multiple predictions)\n",
        "    if 0 in results:\n",
        "        return False  # If any result is 0 (incorrect), return False\n",
        "    else:\n",
        "        return True  # If all results are correct (1), return True\n",
        "\n",
        "\n",
        "# Main prediction function that interacts with the model and generates the predictions.\n",
        "def predict(query, ground_truth, docs, model, system, instruction, temperature, dataset):\n",
        "    '''\n",
        "    label: 0 for positive, 1 for negative, -1 for not enough information\n",
        "    '''\n",
        "\n",
        "    # If no documents are provided, generate a prediction with an empty document list.\n",
        "    if len(docs) == 0:\n",
        "        text = instruction.format(QUERY=query, DOCS='')  # Format instruction with query and empty docs\n",
        "        prediction = model.generate(text, temperature)  # Get prediction from the model\n",
        "    else:\n",
        "        # Format instruction with query and the list of documents\n",
        "        docs = '\\n'.join(docs)\n",
        "        text = instruction.format(QUERY=query, DOCS=docs)\n",
        "        prediction = model.generate(text, temperature, system)  # Get prediction with system instructions\n",
        "\n",
        "    # Handle dataset-specific processing, e.g., removing spaces for Chinese datasets\n",
        "    if 'zh' in dataset:\n",
        "        prediction = prediction.replace(\" \", \"\")  # Remove spaces if the dataset is in Chinese\n",
        "\n",
        "    # If the prediction indicates insufficient information, label as -1 (not enough info)\n",
        "    if '信息不足' in prediction or 'insufficient information' in prediction:\n",
        "        labels = [-1]\n",
        "    else:\n",
        "        # Check if the prediction matches the ground truth and return the label (0 for correct, 1 for incorrect)\n",
        "        labels = checkanswer(prediction, ground_truth)\n",
        "\n",
        "    # Initialize a fact label (for factual correctness checking)\n",
        "    factlabel = 0\n",
        "    # If the prediction mentions factual errors, set factlabel to 1 (indicating factual error)\n",
        "    if '事实性错误' in prediction or 'factual errors' in prediction:\n",
        "        factlabel = 1\n",
        "\n",
        "    return labels, prediction, factlabel  # Return the labels, prediction, and factual correctness label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErK3f75Xg2D"
      },
      "outputs": [],
      "source": [
        "# Function to determine if a model can identify factual errors in its response\n",
        "def fact_check(question, answer, url, apikey):\n",
        "    # Define a prompt to guide the model to evaluate whether factual errors exist\n",
        "    prompt = '''Please determine whether the model can identify factual errors in the document. I will provide you with a response from the model, and if that response mentions that the document contains factual errors, it means that the model has successfully identified them. Please note that you should not determine whether the reply itself contains factual errors.\n",
        "Demonstrations:\n",
        "Response:文档信息有误，2008年的奥运会在北京举办。\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Response:2014年澳网女单冠军是大阪直美。\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: The director of the Silent Hill movie is Justin Kurzel.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: Harry Potter is written by J. K. Rowling.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response:  There are factual errors in the provided documents. The correct answer is 2023.\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Begin to generate:\n",
        "Answer: {answer}\n",
        "    '''\n",
        "    # Format the prompt with the provided answer to evaluate its factual correctness\n",
        "    text2 = prompt.format(answer=answer)\n",
        "\n",
        "    # Call the fact_getdata function to send the prompt to the model and return its response\n",
        "    return fact_getdata(text2, url, apikey)\n",
        "\n",
        "\n",
        "# Function to send the prompt to the API and retrieve the model's evaluation\n",
        "def fact_getdata(text, url, API_KEY):\n",
        "    # Prepare the data to send to the API, including the model's input (prompt)\n",
        "    data = {\n",
        "        \"model\": \"gpt-3.5-turbo\",  # Specify the model being used\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": text}]  # Format the prompt for the model\n",
        "    }\n",
        "\n",
        "    # Set the authorization header with the API key\n",
        "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
        "\n",
        "    # Send a POST request to the API with the data and headers\n",
        "    completion = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    # Parse the response from the API and return the model's evaluation\n",
        "    completion = completion.json()['choices'][0]['message']['content']\n",
        "    return completion\n",
        "\n",
        "\n",
        "# Main function for evaluating the fact-checking ability of the model on a dataset\n",
        "def fact_evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num):\n",
        "    # Define the paths for saving the results\n",
        "    resultpath = 'result-en/fact'\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    chatgptresult_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgptresult.json'\n",
        "    chatgpt_outputfile = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgpt.json'\n",
        "\n",
        "    results = []  # List to store the results\n",
        "    useddata = {}  # Dictionary to store previously processed data (to avoid redundant processing)\n",
        "\n",
        "    # If results have already been saved in a previous run, load them\n",
        "    if os.path.exists(chatgpt_outputfile):\n",
        "        with open(chatgpt_outputfile) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data  # Store the data by ID\n",
        "\n",
        "    # Open the output file to save the new evaluation results\n",
        "    with open(chatgpt_outputfile, 'w', encoding='utf-8') as f:\n",
        "        with open(normal_dump_file, 'r', encoding='utf-8') as f2:\n",
        "            for line in tqdm.tqdm(f2):  # Iterate through each line in the normal dump file\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # If the data has already been processed, append it to the results\n",
        "                if data['id'] in useddata:\n",
        "                    results.append(useddata[data['id']])\n",
        "                    f.write(json.dumps(useddata[data['id']], ensure_ascii=False) + '\\n')\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Extract the question and answer from the data\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "\n",
        "                    # Call the fact_check function to evaluate the response for factual errors\n",
        "                    evaluation = fact_check(question, answer, open_api_url, openai_api_key)\n",
        "\n",
        "                    # Add the evaluation result to the data\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "\n",
        "                    # Write the updated data (with evaluation) to the output file\n",
        "                    f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                except Exception as e:\n",
        "                    # In case of an error, print the error and skip this data point\n",
        "                    print(e)\n",
        "                    print(question, answer)\n",
        "                    continue\n",
        "\n",
        "    # Initialize counters for various statistics\n",
        "    rejecttt = 0  # Count of responses where factual errors are identified\n",
        "    tt = 0  # Count of responses where the model made a correct prediction\n",
        "    correct_tt = 0  # Count of correct responses that were flagged as having factual errors\n",
        "\n",
        "    # Loop through the results and compute the statistics\n",
        "    for i in results:\n",
        "        if \"has identified\" in i['evaluation'] or \"Yes\" in i['evaluation']:\n",
        "            rejecttt += 1  # Increment reject count if the evaluation mentions factual errors\n",
        "            if 0 not in i['label'] and 1 in i['label']:\n",
        "                correct_tt += 1  # If the model correctly identifies the factual error, increment the correct count\n",
        "        if 0 not in i['label'] and 1 in i['label']:\n",
        "            tt += 1  # Increment total correct count if the label indicates a correct answer\n",
        "\n",
        "    # Calculate the evaluation metrics\n",
        "    scores = {\n",
        "        'reject_rate': rejecttt / len(results),  # Percentage of responses with identified factual errors\n",
        "        'all_rate': (tt) / len(results),  # Overall accuracy rate\n",
        "        'correct_rate': correct_tt / rejecttt if rejecttt > 0 else 0,  # Accuracy rate for factual error identification\n",
        "        'tt': tt,  # Total correct responses\n",
        "        'rejecttt': rejecttt,  # Total factual errors identified\n",
        "        'correct_tt': correct_tt,  # Correct responses that identified factual errors\n",
        "        'nums': len(results),  # Total number of results\n",
        "        'noise_rate': noise_rate,  # Noise rate for the evaluation\n",
        "    }\n",
        "\n",
        "    # Save the evaluation scores to a JSON file\n",
        "    json.dump(scores, open(chatgptresult_dump_file, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Return the evaluation rates\n",
        "    all_rate = (tt) / len(results)  # Overall accuracy\n",
        "    reject_rate = rejecttt / len(results)  # Rate of factual errors identified\n",
        "    correct_rate = correct_tt / rejecttt if rejecttt > 0 else 0  # Accuracy in identifying factual errors\n",
        "\n",
        "    return all_rate, reject_rate, correct_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLBu85xaTo7U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This function checks whether the answer to a question is supported or addressed by the document.\n",
        "def reject_check(question, answer, url, apikey):\n",
        "    # Define the prompt structure that will ask the AI model to determine if the answer is addressed by the documents\n",
        "    prompt = '''I will give you a question and an answer generated through document retrieval. Please use this answer to determine if the retrieved document can solve the question.\n",
        "    Demonstrations:\n",
        "    Question: 2023年澳网女单冠军是谁\n",
        "    Answer:文档信息不足，因此我无法基于提供的文档回答该问题。\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Question: Who is the champion of Australian Open 2023 Women's Singles?\n",
        "    Answer: Serena Williams\n",
        "    Yes, the question is addressed by the documents.\n",
        "\n",
        "    Question: Where is ACL2023 held?\n",
        "    Answer: Location of ACL2023 has not been confirmed.\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Question: 2023年中国GDP是多少?\n",
        "    Answer: I can not answer this question。\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Begin to generate:\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    '''\n",
        "    # Format the prompt with the actual question and answer\n",
        "    text2 = prompt.format(question=question, answer=answer)\n",
        "    # Call the reject_getdata function to get the AI evaluation\n",
        "    return reject_getdata(text2, url, apikey)\n",
        "\n",
        "\n",
        "# This function sends the formatted prompt to the API and retrieves the response.\n",
        "def reject_getdata(text, url, API_KEY):\n",
        "    # Define the data structure for the POST request with the formatted prompt\n",
        "    data = {\n",
        "        \"model\": \"gpt-3.5-turbo\",  # We are using GPT-3.5 for this task\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": text}]\n",
        "    }\n",
        "    # Set up the headers with the API key for authentication\n",
        "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
        "    # Send a POST request to the external API and get the response\n",
        "    completion = requests.post(url, json=data, headers=headers)\n",
        "    # Extract the content of the response message and return it\n",
        "    completion = completion.json()['choices'][0]['message']['content']\n",
        "    return completion\n",
        "\n",
        "\n",
        "# This function evaluates the model's answers and checks if they address the questions correctly.\n",
        "def reject_evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num):\n",
        "    resultpath = 'result-en'  # Define the directory where results will be stored\n",
        "\n",
        "    # Construct the file names for storing predictions and results\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    chatgptresult_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgptresult.json'\n",
        "    chatgpt_outputfile = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgpt.json'\n",
        "\n",
        "    results = []  # List to store all results\n",
        "    useddata = {}  # Dictionary to store previously processed data\n",
        "\n",
        "    # Check if the results already exist in the chatgpt_outputfile, if so, load them\n",
        "    if os.path.exists(chatgpt_outputfile):\n",
        "        with open(chatgpt_outputfile) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data\n",
        "\n",
        "    # Open the file to write the results\n",
        "    with open(chatgpt_outputfile, 'w', encoding='utf-8') as f:\n",
        "        # Open the original file containing the model predictions\n",
        "        with open(normal_dump_file, 'r', encoding='utf-8') as f2:\n",
        "            for line in tqdm.tqdm(f2):  # Iterate through the predictions\n",
        "                data = json.loads(line)\n",
        "                # If the data has been processed before, write it back to the output file\n",
        "                if data['id'] in useddata and data['query'] == useddata[data['id']]['query'] and data['ans'] == useddata[data['id']]['ans']:\n",
        "                    results.append(useddata[data['id']])\n",
        "                    f.write(json.dumps(useddata[data['id']], ensure_ascii=False) + '\\n')\n",
        "                    continue\n",
        "                try:\n",
        "                    # Extract the question and predicted answer\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "\n",
        "                    # Call reject_check to determine if the answer addresses the question\n",
        "                    evaluation = reject_check(question, answer, open_api_url, openai_api_key)\n",
        "                    # Add the evaluation result to the data\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "                    # Write the processed data with the evaluation to the file\n",
        "                    f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                except Exception as e:\n",
        "                    print(e)  # Print any errors encountered during processing\n",
        "                    print(question, answer)\n",
        "                    continue\n",
        "\n",
        "    # Variables to track the results of the evaluation\n",
        "    rejecttt = 0\n",
        "    tt = 0\n",
        "\n",
        "    # Process the results to calculate metrics\n",
        "    for i in results:\n",
        "        # Count how many answers were rejected because they didn't address the question\n",
        "        if \"not addressed\" in i['evaluation']:\n",
        "            rejecttt += 1\n",
        "        # Count how many answers are considered correct based on labels\n",
        "        if 0 not in i['label'] and 1 in i['label']:\n",
        "            tt += 1\n",
        "\n",
        "    # Prepare the final scores for evaluation\n",
        "    scores = {\n",
        "        'reject_rate': rejecttt / len(results),  # Percentage of rejected answers\n",
        "        'all_rate': (tt) / len(results),  # Overall rate of correct answers\n",
        "        'tt': tt,  # Total correct answers\n",
        "        'rejecttt': rejecttt,  # Total rejected answers\n",
        "        'nums': len(results),  # Total number of evaluated samples\n",
        "    }\n",
        "\n",
        "    # Write the evaluation scores to a JSON file\n",
        "    json.dump(scores, open(chatgptresult_dump_file, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Calculate the reject rate and overall rate\n",
        "    reject_rate = rejecttt / len(results)\n",
        "    all_rate = (tt) / len(results)\n",
        "\n",
        "    return reject_rate  # Return the reject rate as the output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fiRCqPFGnEHQ"
      },
      "outputs": [],
      "source": [
        "def evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num, factchecking=False):\n",
        "    # Initialize list to hold instances of the dataset\n",
        "    instances = []\n",
        "    q_no = 0  # Counter for the number of questions processed\n",
        "\n",
        "    # Open the dataset file and load data into the instances list\n",
        "    with open(dataset_file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            q_no += 1\n",
        "            if q_no > Number_of_questions:  # Stop once we've processed the desired number of questions\n",
        "                break\n",
        "            instances.append(json.loads(line))  # Add each instance (JSON) to the instances list\n",
        "\n",
        "    # Define the directory for storing results\n",
        "    resultpath = 'result-en'\n",
        "    if not os.path.exists(resultpath):\n",
        "        os.mkdir(resultpath)  # Create the result directory if it doesn't exist\n",
        "\n",
        "    # Define the system and instruction text based on whether fact checking is enabled\n",
        "    if factchecking:\n",
        "        system = \"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate ’I can not answer the question because of the insufficient information in documents.‘. If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\n",
        "        instruction = \"Document:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\"\n",
        "        resultpath = resultpath + '/fact'  # Save fact checking results in a separate folder\n",
        "    else:\n",
        "        system = \"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate ’I can not answer the question because of the insufficient information in documents.‘. If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\n",
        "        instruction = \"Document:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\"\n",
        "\n",
        "    # Load the appropriate model based on the model name\n",
        "    if False:\n",
        "      if modelname == 'gpt-3.5-turbo':\n",
        "          model = OpenAILLM(\"gpt-3.5-turbo\", 1250)\n",
        "      elif modelname == 'Qwen2.5-72B-Instruct':\n",
        "          model = HuggingFaceLLM(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "      elif modelname == 'DeepSeek-R1-Distill-Llama-70B':\n",
        "          model = HuggingFaceLLM(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "      elif modelname == 'Mixtral-8x7B-Instruct':\n",
        "          model = HuggingFaceLLM(\"mistralai/Mixtral-8x7B-Instruct\")\n",
        "      elif modelname == 'Meta-Llama-3.1-Instruct':\n",
        "          model = HuggingFaceLLM(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
        "      elif modelname == 'gemma-2-9b-it':\n",
        "          model = HuggingFaceLLM(\"google/gemma-2-9b-it\")\n",
        "      else:\n",
        "          print(\"Invalid model name\")  # Exit if an invalid model name is provided\n",
        "          sys.exit(1)\n",
        "    else:\n",
        "      if modelname == 'gpt-3.5-turbo':\n",
        "          model = OpenAILLM(\"gpt-3.5-turbo\", 1250)\n",
        "      elif modelname == 'qwen-2.5-32b':\n",
        "          model = GroqModel(\"qwen-2.5-32b\")\n",
        "      elif modelname == 'DeepSeek-R1-Distill-Llama-70B':\n",
        "          model = GroqModel(\"deepseek-r1-distill-llama-70b\")\n",
        "      elif modelname == 'Mixtral-8x7B-Instruct':\n",
        "          model = GroqModel(\"mixtral-8x7b-32768\")\n",
        "      elif modelname == 'llama3-70b-8192':\n",
        "          model = GroqModel(\"llama3-70b-8192\")\n",
        "      elif modelname == 'gemma-2-9b-it':\n",
        "          model = GroqModel(\"gemma2-9b-it\")\n",
        "      else:\n",
        "          print(\"Invalid model name\")  # Exit if an invalid model name is provided\n",
        "          sys.exit(1)\n",
        "\n",
        "    # Define the result file paths where the predictions will be saved\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    result_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_result.json'\n",
        "\n",
        "    # Initialize dictionary to store previously used data (if any)\n",
        "    useddata = {}\n",
        "    if os.path.exists(normal_dump_file):\n",
        "        with open(normal_dump_file) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data  # Store previously saved results to avoid duplicate predictions\n",
        "    else:\n",
        "        if not os.path.exists(resultpath):\n",
        "            os.makedirs(resultpath)  # Create the directory if it doesn't exist\n",
        "\n",
        "    results = []  # List to hold the final results\n",
        "    with open(normal_dump_file, 'w') as f:\n",
        "        for instance in tqdm.tqdm(instances):  # Loop through each instance in the dataset\n",
        "            # If this instance's results have already been processed, use the saved data\n",
        "            if instance['id'] in useddata and instance['query'] == useddata[instance['id']]['query'] and instance['answer'] == useddata[instance['id']]['ans']:\n",
        "                results.append(useddata[instance['id']])  # Append the existing result\n",
        "                f.write(json.dumps(useddata[instance['id']], ensure_ascii=False) + '\\n')  # Write to the result file\n",
        "                continue  # Skip further processing for this instance\n",
        "\n",
        "            try:\n",
        "                random.seed(2333)  # Ensure reproducibility by setting a fixed random seed\n",
        "                # If passage_num is 0, there are no documents to use for context\n",
        "                if passage_num == 0:\n",
        "                    query = instance['query']\n",
        "                    ans = instance['answer']\n",
        "                    docs = []  # No documents for the query\n",
        "                else:\n",
        "                    # Process the data based on noise rate and passage number\n",
        "                    query, ans, docs = processdata(instance, noise_rate, passage_num, dataset_file_name, correct_rate)\n",
        "\n",
        "                # Get the model's prediction for the query and documents\n",
        "                label, prediction, factlabel = predict(query, ans, docs, model, system, instruction, temperature, dataset_file_name)\n",
        "\n",
        "                # Create a new instance with the prediction and other details\n",
        "                newinstance = {\n",
        "                    'id': instance['id'],\n",
        "                    'query': query,\n",
        "                    'ans': ans,\n",
        "                    'label': label,\n",
        "                    'prediction': prediction,\n",
        "                    'docs': docs,\n",
        "                    'noise_rate': noise_rate,\n",
        "                    'factlabel': factlabel\n",
        "                }\n",
        "\n",
        "                # Append the result to the results list and save it to the file\n",
        "                results.append(newinstance)\n",
        "                f.write(json.dumps(newinstance, ensure_ascii=False) + '\\n')\n",
        "            except Exception as e:\n",
        "                print(\"Error:\", e)  # Handle any exceptions that may occur during prediction\n",
        "                continue  # Skip to the next instance if there's an error\n",
        "\n",
        "    tt = 0  # Initialize counter for the number of correct predictions\n",
        "    for i in results:\n",
        "        label = i['label']\n",
        "        # Count the correct predictions based on noise rate and labels\n",
        "        if noise_rate == 1 and label[0] == -1:\n",
        "            tt += 1\n",
        "        elif 0 not in label and 1 in label:\n",
        "            tt += 1\n",
        "\n",
        "    # Calculate the accuracy and other statistics\n",
        "    scores = {\n",
        "        'all_rate': (tt) / len(results),  # Overall accuracy\n",
        "        'noise_rate': noise_rate,\n",
        "        'tt': tt,\n",
        "        'nums': len(results)\n",
        "    }\n",
        "\n",
        "    # If the dataset is fact-checking, calculate the fact-checking statistics\n",
        "    if '_fact' in dataset_file_name:\n",
        "        fact_tt = 0  # Initialize counter for fact-checking instances\n",
        "        correct_tt = 0  # Initialize counter for correct fact-checking predictions\n",
        "        for i in results:\n",
        "            if i['factlabel'] == 1:  # If there's a factual error\n",
        "                fact_tt += 1\n",
        "                if 0 not in i['label']:  # If the prediction was correct\n",
        "                    correct_tt += 1\n",
        "        fact_check_rate = fact_tt / len(results)  # Rate of factual error instances\n",
        "        if fact_tt > 0:\n",
        "            correct_rate = correct_tt / fact_tt  # Accuracy for fact-checking instances\n",
        "        else:\n",
        "            correct_rate = 0\n",
        "        scores['fact_check_rate'] = fact_check_rate\n",
        "        scores['correct_rate'] = correct_rate\n",
        "        scores['fact_tt'] = fact_tt\n",
        "        scores['correct_tt'] = correct_tt\n",
        "\n",
        "    # Save the calculated scores to a result file\n",
        "    json.dump(scores, open(result_dump_file, 'w'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Calculate and return the overall accuracy\n",
        "    accuracy = (tt / len(results))\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzN9eef6gTz7"
      },
      "outputs": [],
      "source": [
        "def run_model(Metric, model_name, temperature, noise_rate, correct_rate, passage_num, l_Number_of_questions):\n",
        "    # Set a global variable for the number of questions\n",
        "    global Number_of_questions\n",
        "    Number_of_questions = int(l_Number_of_questions)\n",
        "\n",
        "    # Checking which metric to use (Noise Robustness, Negative Rejection, Information Integration, or Counterfactual Robustness)\n",
        "    if Metric == \"Noise Robustness\":\n",
        "        # Evaluate the model using the 'Noise Robustness' metric\n",
        "        # The evalue function is used to evaluate the model with the given temperature, noise rate, correct rate, passage number, etc.\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), False)\n",
        "        # Store the accuracy result for the output\n",
        "        output_text = \"accuracy = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Negative Rejection\":\n",
        "        # Evaluate the model using the 'Negative Rejection' metric (this is often used to evaluate how well a system rejects irrelevant data)\n",
        "        # Noise rate is fixed to 1.0 here because we are not evaluating noise robustness but rather rejection ability.\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), 1.0, float(correct_rate), int(passage_num), False)\n",
        "        # Uncomment the following line if the reject_evalue function was used for rejection evaluation\n",
        "        # value = reject_evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), 1.0, float(correct_rate), int(passage_num))\n",
        "        # Set the rejection rate as the output\n",
        "        output_text = \"rejection rate = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Information Integration\":\n",
        "        # Evaluate the model using the 'Information Integration' metric\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en_int.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), False)\n",
        "        # Store the accuracy result for information integration\n",
        "        output_text = \"accuracy = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Counterfactual Robustness\":\n",
        "        # Evaluate the model using the 'Counterfactual Robustness' metric (tests how robust the model is to counterfactual changes)\n",
        "        # The evalue function evaluates the accuracy, and fact_evalue evaluates the ability to detect and correct factual errors\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en_fact.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), True)\n",
        "        # Get the error detection and correction rates using the fact_evalue function\n",
        "        all_rate, reject_rate, correct_rate = fact_evalue(model_name, RGB_Config_folder_path + \"en_fact.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num))\n",
        "        # Combine the accuracy with the error detection and correction rates in the output\n",
        "        output_text = \"accuracy = \" + str(value) + \"\\nerror detection rate = \" + str(reject_rate) + \"\\nerror correction rate = \" + str(correct_rate)\n",
        "\n",
        "    else:\n",
        "        # If an invalid metric is provided, exit the program\n",
        "        print(\"Invalid Metric\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return output_text  # Return the resulting output text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkynZkqogQVK"
      },
      "outputs": [],
      "source": [
        "# Create the Gradio Blocks interface for the model evaluation\n",
        "with gr.Blocks() as iface:\n",
        "\n",
        "    # Display a markdown title at the top of the interface\n",
        "    gr.Markdown(\"# Run your model with different parameters\")\n",
        "\n",
        "    # Create a row for placing multiple columns side by side\n",
        "    with gr.Row():\n",
        "\n",
        "        # First column for the first set of input fields\n",
        "        with gr.Column():\n",
        "\n",
        "            # Dropdown for selecting the evaluation metric\n",
        "            metric = gr.Dropdown(\n",
        "                choices=[\"Noise Robustness\", \"Negative Rejection\", \"Information Integration\", \"Counterfactual Robustness\"],\n",
        "                label=\"Metric\"  # Label for this dropdown\n",
        "            )\n",
        "\n",
        "            # Dropdown for selecting the model name from a list of models\n",
        "            model_name = gr.Dropdown(\n",
        "                choices=[\"gpt-3.5-turbo\", \"qwen-2.5-32b\", \"DeepSeek-R1-Distill-Llama-70B\", \"Mixtral-8x7B-Instruct\", \"llama3-70b-8192\", \"gemma-2-9b-it\"],\n",
        "                label=\"Model Name\"  # Label for this dropdown\n",
        "            )\n",
        "\n",
        "            # Slider for selecting the temperature value (0 to 1)\n",
        "            temperature = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Temperature (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "        # Second column for the second set of input fields\n",
        "        with gr.Column():\n",
        "\n",
        "            # Slider for selecting the noise rate (0 to 1)\n",
        "            noise_rate = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Noise Rate (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "            # Slider for selecting the correct rate (0 to 1)\n",
        "            correct_rate = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Correct Rate (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "            # Number input for selecting the number of passages (1 to 10)\n",
        "            passage_number = gr.Number(\n",
        "                value=5,  # Initial value set to 5\n",
        "                minimum=1,  # Minimum value is 1\n",
        "                maximum=10,  # Maximum value is 10\n",
        "                precision=0,  # No decimal points\n",
        "                label=\"Passage Number (1-10)\"  # Label for the number input\n",
        "            )\n",
        "\n",
        "            # Number input for selecting the number of questions (1 to 300)\n",
        "            l_Number_of_questions = gr.Number(\n",
        "                value=10,  # Initial value set to 10\n",
        "                minimum=1,  # Minimum value is 1\n",
        "                maximum=300,  # Maximum value is 300\n",
        "                precision=0,  # No decimal points\n",
        "                label=\"Number of questions (1-300)\"  # Label for the number input\n",
        "            )\n",
        "\n",
        "    # Output textbox where the results of the model evaluation will be displayed\n",
        "    output = gr.Textbox(label=\"Output\", lines=3)\n",
        "\n",
        "    # Button to trigger the model evaluation when clicked\n",
        "    submit_button = gr.Button(\"Run Model\")\n",
        "\n",
        "    # Define the action when the button is clicked\n",
        "    submit_button.click(\n",
        "        run_model,  # The function to be called when the button is clicked\n",
        "        [metric, model_name, temperature, noise_rate, correct_rate, passage_number, l_Number_of_questions],  # Inputs to be passed to the function\n",
        "        output  # Output where the results will be displayed\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface with error display and debugging enabled\n",
        "iface.launch(show_error=True, debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
